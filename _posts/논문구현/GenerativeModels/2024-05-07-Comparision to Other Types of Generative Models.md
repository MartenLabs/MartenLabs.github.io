---
title: 7. Comparision to Other Types of Generative Models
date: 2024-05-07T19:30:00+09:00
categories: [논문구현, GenerativeModels]
tags:
  [
    GANs,
    Comparision to Other Types of Generative Models,
  ]
pin: true
math: true
mermaid: true
---

## 1. Types of gnerative models 
![](https://tera.dscloud.me:8080/Images/Models/GANs/8.ComparisionGenerativeModels/1.png)

![](https://tera.dscloud.me:8080/Images/Models/GANs/8.ComparisionGenerativeModels/2.png)

<br/>
<br/>

---
### 1. Autoregressive Models

#### 특징
- Autoregressive 모델은 각 데이터 포인트가 이전의 데이터 포인트에 조건부로 의존하는 순차적인 데이터 생성 과정을 모델링 한다.
- PixelCNN과 Transformer가 autoregressive 모델의 예
- 이 모델들은 텍스트, 이미지, 음성 등 다양한 종류의 순차 데이터에 적용될 수 있다.

![](https://tera.dscloud.me:8080/Images/Models/GANs/8.ComparisionGenerativeModels/3.png)

<br/>

#### 수식
$p(x) = \prod_{i=1}^{n} p(x_i|x_1, ..., x_{i-1})$

<br/>

#### 수식 해설
- $x$는 생성하고자 하는 데이터 시퀀스
- $p(x_i|x_1, ..., x_{i-1})$는 이전의 모든 데이터 포인트가 주어졌을 때 현재 데이터 포인트 $x_i$의 조건부 확률
- 이 수식은 전체 데이터 시퀀스 $x$의 확률을, 각 데이터 포인트의 조건부 확률의 곱으로 표현

<br/>

#### PixelRNN
![](https://tera.dscloud.me:8080/Images/Models/GANs/8.ComparisionGenerativeModels/4.png)
![](https://tera.dscloud.me:8080/Images/Models/GANs/8.ComparisionGenerativeModels/5.png)

<br/>

#### 장점:
- **강력한 조건부 생성**: Autoregressive 모델은 주어진 시퀀스의 다음 요소를 예측하는 데 매우 효과적이며, 텍스트나 음악 생성 등에 강력한 성능을 보인다.
- **모델의 유연성**: 다양한 길이의 시퀀스 데이터에 유연하게 적용할 수 있다.

<br/>

#### 단점:
- **순차적 생성 과정**: 데이터의 각 요소를 순차적으로 생성해야 하기 때문에 병렬 처리가 어렵고 생성 속도가 느릴 수 있다.
- **장기 의존성 학습의 어려움**: 특히 긴 시퀀스에서 이전 정보를 유지하고 활용하는 것이 어려울 수 있다.

<br/>
<br/>

---
### 2. VAE (Variational Autoencoders)

#### 특징
- VAE는 입력 데이터를 잠재 공간(latent space)의 분포로 인코딩한 다음, 이 분포로부터 샘플링하여 새로운 데이터를 생성하는 구조를 가진 모델
- VAE는 인코더와 디코더로 구성되어 있으며, 인코더는 데이터를 잠재 변수로 변환하고, 디코더는 잠재 변수로부터 데이터를 재구성한다.
- 잠재 공간의 분포를 강제로 특정 분포(예: 정규 분포)에 가깝게 만드는 정규화 용어가 손실 함수에 포함된다.

<br/>

#### 수식
$\log p(x) \geq \mathbb{E}_{z \sim q(z|x)}[\log p(x|z)] - D_{KL}(q(z|x) \| p(z))$

<br/>

#### 수식 해설
- $log \;p(x)$는 데이터 $x$의 로그 확률을 나타낸다.
- $q(z|x)$는 인코더가 데이터 $x$가 주어졌을 때 잠재 변수 $z$의 분포를 나타낸다.
- $p(x|z)$는 디코더가 잠재 변수 $z$로부터 데이터 $x$를 재구성할 확률을 나타낸다.
- $D_{KL}(q(z|x) \| p(z))$는 인코더가 출력하는 잠재 변수 분포 $q(z|x)$와 사전 정의된 분포 $p(z)$ 간의 Kullback-Leibler 발산. 
    이 항은 잠재 공간을 정규화하고, 인코딩된 잠재 변수가 특정 분포를 따르도록 강제한다.

<br/>

#### 장점:
- **안정적인 학습**: VAE는 상대적으로 학습이 안정적이며, 명시적인 목적 함수를 최적화한다.
- **잠재 공간의 해석 가능성**: VAE는 연속적이고 구조화된 잠재 공간을 학습하여, 이 공간에서의 샘플링과 보간이 의미 있는 변화를 생성할 수 있다.

<br/>

#### 단점:
- **흐릿한 이미지 생성**: VAE는 종종 선명도가 떨어지는 흐릿한 이미지를 생성하는 경향이 있다.
- **복잡한 데이터 모델링 제한**: 복잡한 데이터 분포를 모델링하는 데 있어서의 제한이 있을 수 있다.

<br/>
<br/>

---
### 3. Flow-based Models

#### 특징
- Flow-based 모델은 데이터와 잠재 공간 사이의 결정론적 변환을 사용하여 데이터의 확률 분포를 모델링한다.
- 이 모델들은 변환의 역함수와 야코비안 행렬식을 효율적으로 계산할 수 있어야 한다.
- 데이터의 정확한 로그 확률을 계산할 수 있으며, 샘플링과 밀도 추정 모두에서 효율적이다.

![](https://tera.dscloud.me:8080/Images/Models/GANs/8.ComparisionGenerativeModels/6.png)

![](https://tera.dscloud.me:8080/Images/Models/GANs/8.ComparisionGenerativeModels/7.png)

<br/>

#### 수식
$\log p(x) = \log p(z) + \log \left| \det \left( \frac{\partial f^{-1}}{\partial x} \right) \right|$

<br/>

#### 수식 해설
- $x$는 관측 데이터, $z$는 변환된 데이터(잠재 변수)
- $f^{-1}$은 $z$에서 $x$ 로의 역변환을 나타낸다
- $frac{\partial f^{-1}}{\partial x}$는 역변환의 야코비안 행렬을 나타낸다.
- 이 수식은 데이터 $x$의 로그 확률을, 변환된 데이터 $z$의 로그 확률과 변환 시의 야코비안 행렬식의 로그를 사용하여 표현한다.

<br/>

#### 장점:
- **정확한 로그 우도 계산**: Flow-based 모델은 데이터의 정확한 로그 우도를 계산할 수 있어, 모델의 성능을 명확하게 평가할 수 있다.
- **효율적인 샘플링과 밀도 추정**: 정확한 역변환과 야코비안 행렬식을 통해 효율적인 샘플링과 밀도 추정이 가능하다.

<br/>

#### 단점:
- **계산 비용**: 야코비안 행렬식의 계산이 복잡하고 비용이 많이 들 수 있다.
- **모델 설계의 제약**: 역변환 가능한 구조를 설계하는 것이 어려울 수 있으며, 이는 모델 설계에 제약을 가한다.

<br/>
<br/>

---
### 4. Diffusion Models

#### 특징
- Diffusion 모델은 데이터를 점진적으로 노이즈로 변환하는 과정과 그 역과정을 모델링한다.
- 이 과정은 데이터 분포에서 노이즈 분포로의 전환과 노이즈 분포에서 데이터 분포로의 복원 과정으로 구성된다.
- 복잡한 데이터 분포를 학습할 수 있으며, 특히 이미지 생성에서 높은 품질의 결과를 달성할 수 있다.


![](https://tera.dscloud.me:8080/Images/Models/GANs/8.ComparisionGenerativeModels/8.png)

<br/>

#### 수식
$p(x_{t-1}|x_t) = \mathcal{N}(x_{t-1}; \mu_{\theta}(x_t, t), \Sigma_{\theta}(x_t, t))$

#### 수식 해설
- $x_t$는 시간 $t$에서의 데이터 상태를 나타낸다.
- $\mathcal{N}$은 정규 분포를 나타낸다.
- $\mu_{\theta}$와 $\Sigma_{\theta}$는 모델이 학습하는 평균과 공분산을 나타낸다.
- 이 수식은 시간 $t$에서의 데이터 상태 $x_t$가 주어졌을 때 이전 상태 $x_{t-1}$의 조건부 분포를 나타낸다.

<br/>

#### 장점:
- **고품질 이미지 생성**: 최근 연구에서는 Diffusion 모델이 매우 높은 품질의 이미지를 생성할 수 있음을 보여주었다.
- **다양한 데이터 유형에 적용 가능**: 이미지 뿐만 아니라 음성이나 텍스트와 같은 다양한 유형의 데이터에 적용할 수 있다.

<br/>

#### 단점:
- **긴 생성 시간**: Diffusion 과정은 여러 단계를 거쳐야 하므로 새로운 샘플을 생성하는 데 상당한 시간이 소요될 수 있다.
- **학습 및 최적화의 복잡성**: Diffusion 모델은 구현과 최적화가 상대적으로 복잡할 수 있다.

<br/>
<br/>

---
### 5. GAN (Generative Adversarial Networks)

![](https://tera.dscloud.me:8080/Images/Models/GANs/8.ComparisionGenerativeModels/10.png)

#### 특징
- GAN은 생성자(Generator)와 판별자(Discriminator)라는 두 개의 신경망을 adversarial(적대적) 방식으로 학습시키는 모델
- 생성자는 진짜처럼 보이는 데이터를 생성하려고 시도하고, 판별자는 입력된 데이터가 실제 데이터인지 생성자가 만든 가짜 데이터인지를 구분하려 시도
- 학습 과정은 미니맥스 게임과 유사하며, 생성자와 판별자가 서로를 개선하는 방식으로 진행된다.

<br/>

#### 수식
$min_{G} \max_{D} V(D, G) = \mathbb{E}_{x \sim p_{data}(x)}[\log D(x)] + \mathbb{E}_{z \sim p_{z}(z)}[\log(1 - D(G(z)))]$

<br/>

#### 수식 해설
- $V(D, G)$는 판별자와 생성자 간의 게임의 값 함수
- $x$는 실제 데이터 분포 $p_{data}(x)$에서 가져온 샘플
- $z$는 생성자의 입력으로 사용되는 잡음 분포 $p_z(z)$에서 가져온 샘플
- $D(x)$는 판별자가 실제 데이터 $x$를 진짜로 분류할 확률
- $G(z)$는 잡음 $z$로부터 생성자가 생성한 데이터 샘플
- $D(G(z))$는 생성자가 만든 가짜 데이터를 판별자가 진짜로 분류할 확률
- 판별자 $D$는 $D(x)$를 최대화하고 $D(G(z))$를 최소화하려고 한다.
- 생성자 $G$는 $D(G(z))$를 최대화하여 판별자를 속이려고 한다.

<br/>

#### 장점:
- **고품질 이미지 생성**: GAN은 특히 이미지 생성에서 매우 선명하고 높은 해상도의 결과물을 생성할 수 있다.
- **다양한 변형 모델**: 다양한 응용 분야에 맞게 조정할 수 있는 수많은 변형과 확장이 가능하다 (예: CycleGAN, StyleGAN).

<br/>

#### 단점:
- **학습의 어려움**: GAN의 학습은 불안정하며, 모드 붕괴(mode collapse) 같은 문제를 경험할 수 있다.
- **적절한 평가 기준 부재**: GAN의 성능을 정량적으로 평가하기 어려울 수 있으며, 대부분 질적인 평가에 의존한다.

![](https://tera.dscloud.me:8080/Images/Models/GANs/8.ComparisionGenerativeModels/9.png)


<br/>
<br/>
<br/>

# Addition

마르코프 체인(Markov Chain)은 미래의 상태가 오직 현재 상태에만 의존하여 결정되는 확률적인 과정을 모델링하는 수학적 모델. 
이러한 성질을 마르코프 성질(Markov property)이라고 한다. 마르코프 체인은 과거의 이력이나 경로와 무관하게, 현재 상태가 미래 상태의 전이 확률을 결정한다는 점에서 "기억 없음(memoryless)" 특성을 가지고 있다.

<br/>

### 정의

마르코프 체인은 일련의 상태들과 그 상태들 사이의 전이 확률로 구성된다. 상태 공간(state space)은 마르코프 체인이 취할 수 있는 모든 상태의 집합이며, 전이 확률(transition probability)은 한 상태에서 다른 상태로 이동할 확률을 나타낸다.

<br/>

### 수식

마르코프 체인의 전이 확률은 다음과 같이 표현:

$P(X_{n+1} = x | X_1 = x_1, X_2 = x_2, ..., X_n = x_n) = P(X_{n+1} = x | X_n = x_n)$

여기서 $X_n$은 시간 $n$에서의 상태를 나타낸다. 이 수식은 미래의 상태 $X_{n+1}$이 현재 상태 $X_n$에만 의존하고, 과거의 상태들$(X_1, X_2, ..., X_{n-1})$에는 의존하지 않음을 의미한다.

<br/>

### 예시

- **날씨 모델링**: 오늘의 날씨가 맑음일 때 내일 비가 올 확률, 내일도 맑을 확률 등을 마르코프 체인으로 모델링할 수 있다.
- **보드 게임**: 주사위를 던져서 특정 칸으로 이동하는 게임에서, 다음 칸의 위치가 현재 칸에만 의존하는 경우 마르코프 체인으로 모델링할 수 있다.
- **텍스트 생성**: 단어나 문자의 시퀀스에서 다음 단어(또는 문자)의 선택이 오직 현재 단어(또는 문자)에만 기반하는 경우, 이를 마르코프 체인으로 모델링하여 간단한 텍스트 생성기를 만들 수 있다.

마르코프 체인은 다양한 분야에서 활용. 이는 경제학, 생물학, 컴퓨터 과학, 엔지니어링 등 다양한 영역에서 시스템의 동작을 예측하고 분석하는 데 사용된다.

<br/>

### 마르코프 체인 기반 모델 

- Autoregressive Models (PixelRNN, PixelCNN)


마르코프 체인과 GAN(Generative Adversarial Networks)은 서로 다른 접근 방식을 사용하는 생성 모델링 기법. 마르코프 체인의 한계와 GAN의 등장이 가져온 장점을 이해하기 위해서는 두 모델링 기법의 차이점을 살펴볼 필요가 있다.

<br/>

### 마르코프 체인의 한계

마르코프 체인은 강력한 모델링 도구이지만, 몇 가지 한계가 있다:

1. **기억 없음(Memoryless) 속성**: 마르코프 체인은 현재 상태가 미래 상태의 전이 확률을 결정한다는 "기억 없음" 속성을 가진다. 이는 과거의 상태가 미래 상태에 영향을 미칠 수 있는 복잡한 시스템을 모델링하기에는 제한적일 수 있다.

2. **단순한 전이 확률**: 마르코프 체인은 상태 간의 전이 확률에 의존하여 작동한다. 복잡한 데이터 분포나 고차원 데이터를 모델링하기 위해서는 매우 많은 상태와 전이 확률이 필요하며, 이는 계산적으로 비효율적이거나 실현 불가능할 수 있다.

3. **정적인 모델**: 마르코프 체인은 정적인 전이 확률을 사용한다. 시간에 따라 변화하는 동적인 시스템이나 데이터 분포를 모델링하는 데는 한계가 있다.

<br/>

### GAN의 장점

GAN은 생성자와 판별자라는 두 신경망을 경쟁시키는 방식으로 작동한다. 이 접근 방식은 마르코프 체인이 가진 한계를 극복하는 여러 장점을 가진다:

1. **고차원 및 복잡한 데이터 분포 모델링**: GAN은 이미지, 음성, 텍스트 등 고차원의 복잡한 데이터 분포를 효과적으로 모델링할 수 있다.   생성자는 실제 데이터와 유사한 새로운 데이터 샘플을 생성하고, 판별자는 실제 데이터와 생성된 데이터를 구별하려고 한다.

2. **동적 학습 과정**: GAN의 학습 과정은 동적이다. 생성자와 판별자는 서로를 개선하며 학습하므로, 모델의 성능은 시간에 따라 계속해서 발전한다.

3. **높은 품질의 데이터 생성**: GAN은 매우 선명하고 현실적인 이미지를 생성할 수 있다. 이는 복잡한 데이터 분포 내의 미묘한 특성까지 학습할 수 있기 때문이다.

<br/>

마르코프 체인의 한계를 극복하고 높은 품질의 복잡한 데이터 생성이 가능한 GAN의 등장은 생성 모델링 분야에 혁신을 가져왔다. 
그러나 GAN 역시 학습의 불안정성, 모드 붕괴(mode collapse) 등 자체적인 문제점을 가지고 있으며, 이러한 문제를 해결하기 위한 연구가 지속적으로 이루어지고 있다.